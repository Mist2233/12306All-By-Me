"""
å¸¦ç™»å½•çŠ¶æ€çš„çˆ¬è™«å·¥å…·
æ”¯æŒä¿å­˜å’Œå¤ç”¨æµè§ˆå™¨ç™»å½•çŠ¶æ€ï¼Œç”¨äºçˆ¬å–éœ€è¦ç™»å½•çš„é¡µé¢
"""

from playwright.sync_api import sync_playwright
import json
import os
from pathlib import Path
from site_crawler import SiteCrawler
import time


class AuthenticatedCrawler:
    """æ”¯æŒç™»å½•çŠ¶æ€çš„çˆ¬è™«"""

    def __init__(self, auth_file="auth_state.json"):
        """
        åˆå§‹åŒ–è®¤è¯çˆ¬è™«

        Args:
            auth_file: ä¿å­˜è®¤è¯çŠ¶æ€çš„æ–‡ä»¶è·¯å¾„
        """
        self.auth_file = auth_file
        self.auth_data = None

    def manual_login(
        self, login_url="https://kyfw.12306.cn/otn/resources/login.html", wait_time=120
    ):
        """
        æ‰“å¼€æµè§ˆå™¨è®©ç”¨æˆ·æ‰‹åŠ¨ç™»å½•ï¼Œå¹¶ä¿å­˜ç™»å½•çŠ¶æ€

        Args:
            login_url: ç™»å½•é¡µé¢çš„URL
            wait_time: ç­‰å¾…ç”¨æˆ·ç™»å½•çš„æœ€é•¿æ—¶é—´ï¼ˆç§’ï¼‰

        æµç¨‹:
            1. æ‰“å¼€æµè§ˆå™¨åˆ°ç™»å½•é¡µé¢
            2. ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å®Œæˆç™»å½•
            3. æ£€æµ‹ç™»å½•æˆåŠŸåè‡ªåŠ¨ä¿å­˜çŠ¶æ€
        """
        print("=" * 60)
        print("ğŸ” æ‰‹åŠ¨ç™»å½•å‘å¯¼")
        print("=" * 60)
        print(f"\nğŸ“Œ å³å°†æ‰“å¼€æµè§ˆå™¨ï¼Œè¯·åœ¨ {wait_time} ç§’å†…å®Œæˆç™»å½•æ“ä½œ")
        print("\næ“ä½œæ­¥éª¤ï¼š")
        print("  1. æµè§ˆå™¨ä¼šè‡ªåŠ¨æ‰“å¼€ç™»å½•é¡µé¢")
        print("  2. è¯·æ‰‹åŠ¨è¾“å…¥ç”¨æˆ·åã€å¯†ç å¹¶å®ŒæˆéªŒè¯ç ")
        print("  3. ç™»å½•æˆåŠŸåï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å¯¼èˆªåˆ°ä»»æ„éœ€è¦ç™»å½•çš„é¡µé¢")
        print("  4. çœ‹åˆ°é¡µé¢æ­£å¸¸æ˜¾ç¤ºåï¼Œåœ¨ç»ˆç«¯æŒ‰ Enter é”®ç»§ç»­")
        print("\nâš ï¸  æç¤ºï¼šä¸è¦å…³é—­æµè§ˆå™¨çª—å£ï¼")
        print("=" * 60)

        input("\næŒ‰ Enter é”®å¼€å§‹æ‰“å¼€æµè§ˆå™¨...")

        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨ï¼ˆéæ— å¤´æ¨¡å¼ï¼Œè®©ç”¨æˆ·å¯ä»¥çœ‹åˆ°ï¼‰
            browser = p.chromium.launch(headless=False)
            context = browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            )

            page = context.new_page()

            try:
                # æ‰“å¼€ç™»å½•é¡µé¢
                print(f"\nğŸŒ æ­£åœ¨æ‰“å¼€ç™»å½•é¡µé¢: {login_url}")
                page.goto(login_url, wait_until="networkidle")

                print("\nâœ… æµè§ˆå™¨å·²æ‰“å¼€ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š")
                print("   1ï¸âƒ£  è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ")
                print("   2ï¸âƒ£  å®ŒæˆéªŒè¯ç éªŒè¯")
                print("   3ï¸âƒ£  ç‚¹å‡»ç™»å½•æŒ‰é’®")
                print("   4ï¸âƒ£  ç¡®è®¤ç™»å½•æˆåŠŸï¼ˆé¡µé¢è·³è½¬æˆ–æ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯ï¼‰")
                print("   5ï¸âƒ£  å¯é€‰ï¼šè®¿é—®ä¸€ä¸ªéœ€è¦ç™»å½•çš„é¡µé¢æµ‹è¯•")

                # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•
                input("\nâœ‹ ç™»å½•å®Œæˆåï¼ŒæŒ‰ Enter é”®ä¿å­˜ç™»å½•çŠ¶æ€...")

                # ä¿å­˜è®¤è¯çŠ¶æ€ï¼ˆåŒ…æ‹¬cookieså’ŒlocalStorageï¼‰
                print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç™»å½•çŠ¶æ€...")

                # ä¿å­˜cookies
                cookies = context.cookies()

                # ä¿å­˜localStorageï¼ˆå¯èƒ½åŒ…å«tokenç­‰ï¼‰
                local_storage = page.evaluate(
                    """() => {
                    let items = {};
                    for (let i = 0; i < localStorage.length; i++) {
                        let key = localStorage.key(i);
                        items[key] = localStorage.getItem(key);
                    }
                    return items;
                }"""
                )

                # ä¿å­˜sessionStorage
                session_storage = page.evaluate(
                    """() => {
                    let items = {};
                    for (let i = 0; i < sessionStorage.length; i++) {
                        let key = sessionStorage.key(i);
                        items[key] = sessionStorage.getItem(key);
                    }
                    return items;
                }"""
                )

                # ç»„åˆæ‰€æœ‰è®¤è¯æ•°æ®
                auth_data = {
                    "cookies": cookies,
                    "local_storage": local_storage,
                    "session_storage": session_storage,
                    "timestamp": time.time(),
                }

                # ä¿å­˜åˆ°æ–‡ä»¶
                with open(self.auth_file, "w", encoding="utf-8") as f:
                    json.dump(auth_data, f, indent=2, ensure_ascii=False)

                print(f"âœ… ç™»å½•çŠ¶æ€å·²ä¿å­˜åˆ°: {self.auth_file}")
                print(f"   - Cookies: {len(cookies)} ä¸ª")
                print(f"   - LocalStorage: {len(local_storage)} é¡¹")
                print(f"   - SessionStorage: {len(session_storage)} é¡¹")

                # éªŒè¯ç™»å½•çŠ¶æ€
                print("\nğŸ” æ­£åœ¨éªŒè¯ç™»å½•çŠ¶æ€...")
                test_url = input(
                    "è¯·è¾“å…¥ä¸€ä¸ªéœ€è¦ç™»å½•çš„é¡µé¢URLè¿›è¡Œæµ‹è¯•ï¼ˆæŒ‰Enterè·³è¿‡ï¼‰: "
                ).strip()

                if test_url:
                    page.goto(test_url, wait_until="networkidle")
                    print("âœ… è¯·åœ¨æµè§ˆå™¨ä¸­æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£å¸¸æ˜¾ç¤º")
                    input("ç¡®è®¤æ— è¯¯åæŒ‰ Enter å…³é—­æµè§ˆå™¨...")

                print(
                    "\nâœ… ç™»å½•çŠ¶æ€ä¿å­˜æˆåŠŸï¼ç°åœ¨å¯ä»¥ä½¿ç”¨ crawl_with_auth() æ–¹æ³•çˆ¬å–éœ€è¦ç™»å½•çš„é¡µé¢äº†ã€‚"
                )

            except Exception as e:
                print(f"\nâŒ ä¿å­˜ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}")
                raise
            finally:
                browser.close()

    def load_auth_state(self):
        """
        åŠ è½½ä¿å­˜çš„è®¤è¯çŠ¶æ€

        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if not os.path.exists(self.auth_file):
            print(f"âŒ è®¤è¯æ–‡ä»¶ä¸å­˜åœ¨: {self.auth_file}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œ manual_login() æ–¹æ³•è¿›è¡Œç™»å½•")
            return False

        try:
            with open(self.auth_file, "r", encoding="utf-8") as f:
                self.auth_data = json.load(f)

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆä¾‹å¦‚7å¤©ï¼‰
            saved_time = self.auth_data.get("timestamp", 0)
            if time.time() - saved_time > 7 * 24 * 3600:
                print("âš ï¸  è­¦å‘Šï¼šç™»å½•çŠ¶æ€å¯èƒ½å·²è¿‡æœŸï¼ˆè¶…è¿‡7å¤©ï¼‰ï¼Œå»ºè®®é‡æ–°ç™»å½•")

            print(
                f"âœ… å·²åŠ è½½ç™»å½•çŠ¶æ€: {len(self.auth_data.get('cookies', []))} ä¸ªcookies"
            )
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½è®¤è¯çŠ¶æ€å¤±è´¥: {e}")
            return False

    def crawl_with_auth(self, urls, output_dir="deconstructed_site"):
        """
        ä½¿ç”¨ä¿å­˜çš„ç™»å½•çŠ¶æ€çˆ¬å–é¡µé¢

        Args:
            urls: è¦çˆ¬å–çš„URLåˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
            output_dir: è¾“å‡ºç›®å½•
        """
        # ç¡®ä¿åŠ è½½äº†è®¤è¯çŠ¶æ€
        if not self.auth_data:
            if not self.load_auth_state():
                print("\nâŒ æ— æ³•åŠ è½½ç™»å½•çŠ¶æ€ï¼Œè¯·å…ˆè¿è¡Œ manual_login() æ–¹æ³•")
                return

        # å¦‚æœæ˜¯å•ä¸ªURLï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(urls, str):
            urls = [urls]

        print(f"\nğŸš€ å¼€å§‹çˆ¬å– {len(urls)} ä¸ªéœ€è¦ç™»å½•çš„é¡µé¢...")

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)

            # åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡
            context = browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            )

            try:
                # æ³¨å…¥cookies
                context.add_cookies(self.auth_data.get("cookies", []))
                print(f"âœ… å·²æ³¨å…¥ {len(self.auth_data.get('cookies', []))} ä¸ªcookies")

                page = context.new_page()

                # æ³¨å…¥localStorageå’ŒsessionStorage
                if self.auth_data.get("local_storage"):
                    page.evaluate(
                        f"""(items) => {{
                        for (let key in items) {{
                            localStorage.setItem(key, items[key]);
                        }}
                    }}""",
                        self.auth_data["local_storage"],
                    )
                    print(f"âœ… å·²æ³¨å…¥ LocalStorage")

                if self.auth_data.get("session_storage"):
                    page.evaluate(
                        f"""(items) => {{
                        for (let key in items) {{
                            sessionStorage.setItem(key, items[key]);
                        }}
                    }}""",
                        self.auth_data["session_storage"],
                    )
                    print(f"âœ… å·²æ³¨å…¥ SessionStorage")

                # ä½¿ç”¨ SiteCrawler çš„é€»è¾‘çˆ¬å–é¡µé¢
                for i, url in enumerate(urls, 1):
                    print(f"\n[{i}/{len(urls)}] æ­£åœ¨çˆ¬å–: {url}")
                    try:
                        page.goto(url, wait_until="networkidle", timeout=60000)

                        # ç­‰å¾…é¡µé¢åŠ è½½
                        page.wait_for_timeout(3000)

                        # æ£€æŸ¥æ˜¯å¦ä»ç„¶ç™»å½•ï¼ˆå¯é€‰ï¼‰
                        # è¿™é‡Œå¯ä»¥æ·»åŠ æ£€æŸ¥é€»è¾‘ï¼Œä¾‹å¦‚æŸ¥æ‰¾ç™»å½•çŠ¶æ€æ ‡è¯†å…ƒç´ 

                        # ä¸ºæ¯ä¸ªURLåˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹æ¥å¤„ç†
                        crawler = SiteCrawler(url, output_dir, max_pages=1)

                        # ç”Ÿæˆé¡µé¢ç›®å½•
                        page_dir_name = crawler._get_page_dir_name(url)
                        page_dir = os.path.join(output_dir, page_dir_name)

                        # è§£æ„é¡µé¢ï¼ˆä½¿ç”¨ SiteCrawler çš„æ–¹æ³•ï¼‰
                        result, _ = crawler._deconstruct_single_page(
                            page, url, page_dir
                        )

                        if result:
                            print(f"âœ… æˆåŠŸä¿å­˜é¡µé¢åˆ°: {page_dir}")
                        else:
                            print(f"âš ï¸  é¡µé¢ä¿å­˜å¤±è´¥")

                    except Exception as e:
                        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
                        continue

                print(f"\nâœ… çˆ¬å–å®Œæˆï¼è¾“å‡ºç›®å½•: {output_dir}")

            except Exception as e:
                print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
                raise
            finally:
                browser.close()


def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    print(
        """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          12306 è®¤è¯çˆ¬è™«å·¥å…· - ä½¿ç”¨è¯´æ˜                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Œ ä½¿ç”¨æµç¨‹ï¼š

ç¬¬ä¸€æ­¥ï¼šä¿å­˜ç™»å½•çŠ¶æ€
    crawler = AuthenticatedCrawler()
    crawler.manual_login()
    # æŒ‰ç…§æç¤ºåœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•

ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ç™»å½•çŠ¶æ€çˆ¬å–é¡µé¢
    crawler.crawl_with_auth([
        "https://kyfw.12306.cn/otn/leftTicket/init",  # è½¦ç¥¨æŸ¥è¯¢é¡µ
        "https://kyfw.12306.cn/otn/view/passengers.html",  # å¸¸ç”¨è”ç³»äºº
        "https://kyfw.12306.cn/otn/queryOrder/initNoComplete",  # æœªå®Œæˆè®¢å•
    ])

ğŸ’¡ æç¤ºï¼š
  - ç™»å½•çŠ¶æ€ä¼šä¿å­˜åœ¨ auth_state.json æ–‡ä»¶ä¸­
  - ä¸€æ¬¡ç™»å½•ï¼Œå¯ä»¥é‡å¤ä½¿ç”¨ï¼ˆç›´åˆ°cookieè¿‡æœŸï¼‰
  - å¦‚æœçˆ¬å–æ—¶é‡åˆ°éœ€è¦é‡æ–°ç™»å½•ï¼Œå†æ¬¡è¿è¡Œ manual_login()
"""
    )

    # äº¤äº’å¼èœå•
    crawler = AuthenticatedCrawler()

    while True:
        print("\n" + "=" * 60)
        print("è¯·é€‰æ‹©æ“ä½œï¼š")
        print("  1. æ‰‹åŠ¨ç™»å½•å¹¶ä¿å­˜çŠ¶æ€")
        print("  2. çˆ¬å–éœ€è¦ç™»å½•çš„é¡µé¢")
        print("  3. æµ‹è¯•ç™»å½•çŠ¶æ€")
        print("  4. é€€å‡º")
        print("=" * 60)

        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-4): ").strip()

        if choice == "1":
            # æ‰‹åŠ¨ç™»å½•
            crawler.manual_login()

        elif choice == "2":
            # çˆ¬å–é¡µé¢
            print("\nè¯·è¾“å…¥è¦çˆ¬å–çš„URLï¼ˆä¸€è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
            print("ç¤ºä¾‹ï¼š")
            print("  https://kyfw.12306.cn/otn/leftTicket/init")
            print("  https://kyfw.12306.cn/otn/view/passengers.html")
            print()

            urls = []
            while True:
                url = input("URL: ").strip()
                if not url:
                    break
                urls.append(url)

            if urls:
                output_dir = (
                    input("\nè¾“å‡ºç›®å½• (é»˜è®¤: deconstructed_site): ").strip()
                    or "deconstructed_site"
                )
                crawler.crawl_with_auth(urls, output_dir)
            else:
                print("âŒ æœªè¾“å…¥ä»»ä½•URL")

        elif choice == "3":
            # æµ‹è¯•ç™»å½•çŠ¶æ€
            if crawler.load_auth_state():
                print("\nâœ… ç™»å½•çŠ¶æ€æœ‰æ•ˆ")
                print(
                    f"   ä¿å­˜æ—¶é—´: {time.ctime(crawler.auth_data.get('timestamp', 0))}"
                )
            else:
                print("\nâŒ ç™»å½•çŠ¶æ€æ— æ•ˆæˆ–ä¸å­˜åœ¨")

        elif choice == "4":
            print("\nğŸ‘‹ å†è§ï¼")
            break

        else:
            print("\nâŒ æ— æ•ˆçš„é€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥")


if __name__ == "__main__":
    main()
