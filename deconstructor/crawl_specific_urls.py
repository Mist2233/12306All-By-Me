"""
æ‰¹é‡çˆ¬å–æŒ‡å®šçš„URLåˆ—è¡¨
ä½¿ç”¨åŸæœ‰çš„ SiteCrawler å•é¡µçˆ¬å–åŠŸèƒ½
"""

from playwright.sync_api import sync_playwright
from site_crawler import SiteCrawler
import sys
import os


def crawl_specific_urls(urls, output_dir="deconstructed_site"):
    """
    çˆ¬å–æŒ‡å®šçš„URLåˆ—è¡¨

    Args:
        urls: è¦çˆ¬å–çš„URLåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    print("=" * 80)
    print("æ‰¹é‡çˆ¬å–æŒ‡å®šURL")
    print("=" * 80)
    print(f"\nğŸ“‹ å¾…çˆ¬å–URLæ•°é‡: {len(urls)}")
    for i, url in enumerate(urls, 1):
        print(f"   {i}. {url}")
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 80 + "\n")

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªURLä½œä¸ºbase_urlåˆå§‹åŒ–çˆ¬è™«
    crawler = SiteCrawler(urls[0], output_dir, max_pages=len(urls))

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,  # ä½¿ç”¨æ— å¤´æ¨¡å¼
            args=["--disable-blink-features=AutomationControlled"],  # é¿å…è¢«æ£€æµ‹
        )
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        page = context.new_page()

        successful = 0
        failed = 0

        for i, url in enumerate(urls, 1):
            print("\n" + "â–¶" * 40)
            print(f"ğŸŒ [{i}/{len(urls)}] æ­£åœ¨çˆ¬å–: {url}")
            print("â–¶" * 40 + "\n")

            try:
                # è®¿é—®é¡µé¢
                page.goto(url, wait_until="networkidle", timeout=30000)
                print(f"âœ… é¡µé¢åŠ è½½å®Œæˆ: {page.url}")

                # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿èµ„æºåŠ è½½
                page.wait_for_timeout(2000)

                # ç”Ÿæˆé¡µé¢ç›®å½•å
                page_dir = crawler._get_page_dir_name(page.url)
                page_output_dir = os.path.join(crawler.output_dir, page_dir)

                # ä½¿ç”¨çˆ¬è™«çš„å•é¡µè§£æ„åŠŸèƒ½
                result, new_links = crawler._deconstruct_single_page(
                    page, page.url, page_output_dir
                )

                if result:
                    print("\nâœ… é¡µé¢è§£æ„æˆåŠŸ!")
                    print(f"   ğŸ“„ HTML: {result['html_file']}")
                    print(f"   ğŸ¨ CSS: {len(result['css_files'])} ä¸ªæ–‡ä»¶")
                    print(f"   ğŸ“œ JS: {len(result['js_files'])} ä¸ªæ–‡ä»¶")
                    print(f"   ğŸ–¼ï¸  å›¾ç‰‡: {len(result['images'])} ä¸ªæ–‡ä»¶")
                    print(f"   ğŸ”¤ å­—ä½“: {len(result['fonts'])} ä¸ªæ–‡ä»¶")

                    # æ·»åŠ åˆ°å·²è®¿é—®URLå’Œç»“æœåˆ—è¡¨
                    crawler.visited_urls.add(page.url)
                    crawler.all_results.append(result)
                    successful += 1
                else:
                    print("\nâš ï¸  é¡µé¢è§£æ„è¿”å›ç©ºç»“æœ")
                    failed += 1

            except Exception as e:
                print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
                failed += 1

        browser.close()

    # ç”Ÿæˆsitemap
    print("\n" + "=" * 80)
    print("ğŸ“Š ç”Ÿæˆç«™ç‚¹åœ°å›¾...")
    crawler._generate_sitemap()

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰¹é‡çˆ¬å–å®Œæˆ!")
    print("=" * 80)
    print(f"âœ… æˆåŠŸ: {successful}/{len(urls)}")
    print(f"âŒ å¤±è´¥: {failed}/{len(urls)}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
    print("=" * 80 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    # å®šä¹‰è¦çˆ¬å–çš„URLåˆ—è¡¨
    urls = [
        "https://www.12306.cn/index/",
        "https://kyfw.12306.cn/otn/resources/login.html",
        "https://kyfw.12306.cn/otn/regist/init",
    ]

    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    output_dir = sys.argv[1] if len(sys.argv) > 1 else "deconstructed_site"

    # æ‰§è¡Œçˆ¬å–
    crawl_specific_urls(urls, output_dir)


if __name__ == "__main__":
    main()
